{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-MEDM \"Real World\" Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using SparseArrays\n",
    "using Kronecker\n",
    "using LinearAlgebra\n",
    "using SuiteSparse\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-MEDM Problem and Data\n",
    "\n",
    "- We have some census microdata (PUMS) for Boulder, Colorado from the 2016 American Community Survey 5-Year Estimates. The microdata consists of with $n$ = 5995 respondents and $N$ = 121,708 people.\n",
    "- Boulder is represented by a Public-Use Microdata Area (PUMA) consisting of 84 block groups and 27 census tracts.\n",
    "- **PROBLEM**: PUMS provides no information on the block groups and census tracts in which each \"person\" (sample weight unit) in the PUMS resides. \n",
    "\n",
    "**Solution: Iterative Proportional Fitting (IPF).** Use census data published for two spatial scales, the target units (block groups) and an upper level (tracts, 27 units), known as _constraints_ to make a \"guess\" at where people described by each response type belong. This is done by comparing the published constraints to synthetic constraints reconstructed from each \"guess\" at the probabilities of people's locations given by P-MEDM. The synthetic constraints are updated iteratively until a \"best guess\" is reached. \n",
    "\n",
    "But...**another problem!** Our census data comes from the American Community Survey (ACS). ACS data is updated annually but is inherently uncertain because it is based on a sample vs. a complete count of the population (i.e., the Decennial Census). \n",
    "\n",
    "**Penalized Maximum-Entropy Dasymetric Modeling (P-MEDM)** is an IPF technique specialized to deal with uncertain census data like the ACS (Leyk et al xx; Nagle et al 2014). Besides taking errors between the published and synthetic constraints into account, P-MEDM also accounts for _error variances_. The more \"in the ballpark\" of the published constraints and their variances the synthetic constraints are, the better the solution. Conversely, if a synthetic constraint is out of the range of probable values (i.e., 90% margin of error) of a published constraint, the more _penalized_ the solution will be.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in data\n",
    "constraints_ind = CSV.read(\"data/boulder_constraints_ind_2016_person.csv\")\n",
    "constraints_bg = CSV.read(\"data/boulder_constraints_bg_2016_person.csv\")\n",
    "constraints_trt = CSV.read(\"data/boulder_constraints_trt_2016_person.csv\");\n",
    "constraints_ind = constraints_ind[:,2:ncol(constraints_ind)]\n",
    "constraints_bg = constraints_bg[:,2:ncol(constraints_bg)]\n",
    "constraints_trt = constraints_trt[:,2:ncol(constraints_trt)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **individual-level** constraints are derived from PUMS data. They consist of: \n",
    "\n",
    "- A unique ID for each response (`pid`).\n",
    "- The sample weight (`wt`), which estimates how many people in the PUMA are described by each PUMS response.\n",
    "- Binary-encoded constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>pid</th><th>wt</th><th>POP</th><th>HH</th><th>AGE5U</th><th>AGE18U</th><th>AGE65O</th><th>NFAM</th><th>NFAMALONE</th><th>POV</th><th>RACWHITE</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>5 rows × 17 columns (omitted printing of 6 columns)</p><tr><th>1</th><td>69a</td><td>13</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><th>2</th><td>69b</td><td>10</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><th>3</th><td>1209</td><td>15</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><th>4</th><td>1417a</td><td>20</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><th>5</th><td>1417b</td><td>21</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccccc}\n",
       "\t& pid & wt & POP & HH & AGE5U & AGE18U & AGE65O & NFAM & NFAMALONE & POV & RACWHITE & \\\\\n",
       "\t\\hline\n",
       "\t& String & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 69a & 13 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & $\\dots$ \\\\\n",
       "\t2 & 69b & 10 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & $\\dots$ \\\\\n",
       "\t3 & 1209 & 15 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & $\\dots$ \\\\\n",
       "\t4 & 1417a & 20 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & $\\dots$ \\\\\n",
       "\t5 & 1417b & 21 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×17 DataFrame. Omitted printing of 9 columns\n",
       "│ Row │ pid    │ wt    │ POP   │ HH    │ AGE5U │ AGE18U │ AGE65O │ NFAM  │\n",
       "│     │ \u001b[90mString\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m  │ \u001b[90mInt64\u001b[39m  │ \u001b[90mInt64\u001b[39m │\n",
       "├─────┼────────┼───────┼───────┼───────┼───────┼────────┼────────┼───────┤\n",
       "│ 1   │ 69a    │ 13    │ 1     │ 1     │ 0     │ 0      │ 0      │ 1     │\n",
       "│ 2   │ 69b    │ 10    │ 1     │ 1     │ 0     │ 0      │ 0      │ 1     │\n",
       "│ 3   │ 1209   │ 15    │ 1     │ 0     │ 0     │ 0      │ 0      │ 0     │\n",
       "│ 4   │ 1417a  │ 20    │ 1     │ 1     │ 0     │ 0      │ 0      │ 1     │\n",
       "│ 5   │ 1417b  │ 21    │ 1     │ 1     │ 0     │ 0      │ 0      │ 1     │"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview individual-level constraints\n",
    "first(constraints_ind, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **geographic constraints** at the target/upper levels are derived from the ACS Summary File. They consist of: \n",
    "\n",
    "- The geographic identifier for each census unit (`GEOID`).\n",
    "- The constraint estimates (i.e., `POP`).\n",
    "- The standard errors on the constraints, denoted here with a trailing `s`, (i.e., `POPs`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>GEOID</th><th>POP</th><th>POPs</th><th>HH</th><th>HHs</th><th>AGE5U</th><th>AGE5Us</th><th>AGE18U</th><th>AGE18Us</th><th>AGE65O</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Int64</th></tr></thead><tbody><p>5 rows × 31 columns (omitted printing of 21 columns)</p><tr><th>1</th><td>80130121011</td><td>960</td><td>176.9</td><td>960</td><td>176.9</td><td>47</td><td>33.5009</td><td>156</td><td>59.3071</td><td>12</td></tr><tr><th>2</th><td>80130121012</td><td>1470</td><td>139.818</td><td>1470</td><td>139.818</td><td>98</td><td>44.3727</td><td>325</td><td>79.2818</td><td>288</td></tr><tr><th>3</th><td>80130121013</td><td>1137</td><td>135.562</td><td>1127</td><td>134.954</td><td>76</td><td>29.7376</td><td>257</td><td>53.1732</td><td>140</td></tr><tr><th>4</th><td>80130121014</td><td>1078</td><td>169.605</td><td>1078</td><td>169.605</td><td>14</td><td>14.9524</td><td>284</td><td>60.6168</td><td>54</td></tr><tr><th>5</th><td>80130121021</td><td>1500</td><td>207.903</td><td>1500</td><td>207.903</td><td>107</td><td>39.5464</td><td>253</td><td>67.8568</td><td>92</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccccc}\n",
       "\t& GEOID & POP & POPs & HH & HHs & AGE5U & AGE5Us & AGE18U & AGE18Us & AGE65O & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Float64 & Int64 & Float64 & Int64 & Float64 & Int64 & Float64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 80130121011 & 960 & 176.9 & 960 & 176.9 & 47 & 33.5009 & 156 & 59.3071 & 12 & $\\dots$ \\\\\n",
       "\t2 & 80130121012 & 1470 & 139.818 & 1470 & 139.818 & 98 & 44.3727 & 325 & 79.2818 & 288 & $\\dots$ \\\\\n",
       "\t3 & 80130121013 & 1137 & 135.562 & 1127 & 134.954 & 76 & 29.7376 & 257 & 53.1732 & 140 & $\\dots$ \\\\\n",
       "\t4 & 80130121014 & 1078 & 169.605 & 1078 & 169.605 & 14 & 14.9524 & 284 & 60.6168 & 54 & $\\dots$ \\\\\n",
       "\t5 & 80130121021 & 1500 & 207.903 & 1500 & 207.903 & 107 & 39.5464 & 253 & 67.8568 & 92 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×31 DataFrame. Omitted printing of 24 columns\n",
       "│ Row │ GEOID       │ POP   │ POPs    │ HH    │ HHs     │ AGE5U │ AGE5Us  │\n",
       "│     │ \u001b[90mInt64\u001b[39m       │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────────┼───────┼─────────┼───────┼─────────┼───────┼─────────┤\n",
       "│ 1   │ 80130121011 │ 960   │ 176.9   │ 960   │ 176.9   │ 47    │ 33.5009 │\n",
       "│ 2   │ 80130121012 │ 1470  │ 139.818 │ 1470  │ 139.818 │ 98    │ 44.3727 │\n",
       "│ 3   │ 80130121013 │ 1137  │ 135.562 │ 1127  │ 134.954 │ 76    │ 29.7376 │\n",
       "│ 4   │ 80130121014 │ 1078  │ 169.605 │ 1078  │ 169.605 │ 14    │ 14.9524 │\n",
       "│ 5   │ 80130121021 │ 1500  │ 207.903 │ 1500  │ 207.903 │ 107   │ 39.5464 │"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview target (block-group) level constraints\n",
    "first(constraints_bg, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>GEOID</th><th>POP</th><th>POPs</th><th>HH</th><th>HHs</th><th>AGE5U</th><th>AGE5Us</th><th>AGE18U</th><th>AGE18Us</th><th>AGE65O</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Int64</th></tr></thead><tbody><p>5 rows × 31 columns (omitted printing of 21 columns)</p><tr><th>1</th><td>8013012101</td><td>4645</td><td>207.903</td><td>4635</td><td>208.511</td><td>235</td><td>65.5408</td><td>1022</td><td>124.29</td><td>494</td></tr><tr><th>2</th><td>8013012102</td><td>7458</td><td>313.678</td><td>7279</td><td>309.422</td><td>420</td><td>86.2215</td><td>1377</td><td>160.509</td><td>843</td></tr><tr><th>3</th><td>8013012103</td><td>3953</td><td>165.35</td><td>3931</td><td>167.173</td><td>111</td><td>32.2418</td><td>776</td><td>91.9884</td><td>539</td></tr><tr><th>4</th><td>8013012104</td><td>2424</td><td>123.404</td><td>2414</td><td>123.404</td><td>87</td><td>46.0604</td><td>586</td><td>86.1679</td><td>498</td></tr><tr><th>5</th><td>8013012105</td><td>5604</td><td>299.696</td><td>5594</td><td>300.304</td><td>362</td><td>78.758</td><td>1352</td><td>196.718</td><td>637</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccccc}\n",
       "\t& GEOID & POP & POPs & HH & HHs & AGE5U & AGE5Us & AGE18U & AGE18Us & AGE65O & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Float64 & Int64 & Float64 & Int64 & Float64 & Int64 & Float64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 8013012101 & 4645 & 207.903 & 4635 & 208.511 & 235 & 65.5408 & 1022 & 124.29 & 494 & $\\dots$ \\\\\n",
       "\t2 & 8013012102 & 7458 & 313.678 & 7279 & 309.422 & 420 & 86.2215 & 1377 & 160.509 & 843 & $\\dots$ \\\\\n",
       "\t3 & 8013012103 & 3953 & 165.35 & 3931 & 167.173 & 111 & 32.2418 & 776 & 91.9884 & 539 & $\\dots$ \\\\\n",
       "\t4 & 8013012104 & 2424 & 123.404 & 2414 & 123.404 & 87 & 46.0604 & 586 & 86.1679 & 498 & $\\dots$ \\\\\n",
       "\t5 & 8013012105 & 5604 & 299.696 & 5594 & 300.304 & 362 & 78.758 & 1352 & 196.718 & 637 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×31 DataFrame. Omitted printing of 24 columns\n",
       "│ Row │ GEOID      │ POP   │ POPs    │ HH    │ HHs     │ AGE5U │ AGE5Us  │\n",
       "│     │ \u001b[90mInt64\u001b[39m      │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼────────────┼───────┼─────────┼───────┼─────────┼───────┼─────────┤\n",
       "│ 1   │ 8013012101 │ 4645  │ 207.903 │ 4635  │ 208.511 │ 235   │ 65.5408 │\n",
       "│ 2   │ 8013012102 │ 7458  │ 313.678 │ 7279  │ 309.422 │ 420   │ 86.2215 │\n",
       "│ 3   │ 8013012103 │ 3953  │ 165.35  │ 3931  │ 167.173 │ 111   │ 32.2418 │\n",
       "│ 4   │ 8013012104 │ 2424  │ 123.404 │ 2414  │ 123.404 │ 87    │ 46.0604 │\n",
       "│ 5   │ 8013012105 │ 5604  │ 299.696 │ 5594  │ 300.304 │ 362   │ 78.758  │"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview upper (tract) level constraints\n",
    "first(constraints_trt, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full set of constraints for this example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: constraints not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: constraints not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[7]:3"
     ]
    }
   ],
   "source": [
    "# placeholder, add table/key\n",
    "is_constraint = [!endswith(i, 's') && i != \"GEOID\" for i in names(constraints_bg)]\n",
    "names(constraints_bg)[constraints, :]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the target/upper levels are nested, we need a crosswalk (`geo_lookup`) to link them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>bg</th><th>trt</th></tr><tr><th></th><th>String</th><th>String</th></tr></thead><tbody><p>5 rows × 2 columns</p><tr><th>1</th><td>80130121011</td><td>8013012101</td></tr><tr><th>2</th><td>80130121012</td><td>8013012101</td></tr><tr><th>3</th><td>80130121013</td><td>8013012101</td></tr><tr><th>4</th><td>80130121014</td><td>8013012101</td></tr><tr><th>5</th><td>80130121021</td><td>8013012102</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& bg & trt\\\\\n",
       "\t\\hline\n",
       "\t& String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & 80130121011 & 8013012101 \\\\\n",
       "\t2 & 80130121012 & 8013012101 \\\\\n",
       "\t3 & 80130121013 & 8013012101 \\\\\n",
       "\t4 & 80130121014 & 8013012101 \\\\\n",
       "\t5 & 80130121021 & 8013012102 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×2 DataFrame\n",
       "│ Row │ bg          │ trt        │\n",
       "│     │ \u001b[90mString\u001b[39m      │ \u001b[90mString\u001b[39m     │\n",
       "├─────┼─────────────┼────────────┤\n",
       "│ 1   │ 80130121011 │ 8013012101 │\n",
       "│ 2   │ 80130121012 │ 8013012101 │\n",
       "│ 3   │ 80130121013 │ 8013012101 │\n",
       "│ 4   │ 80130121014 │ 8013012101 │\n",
       "│ 5   │ 80130121021 │ 8013012102 │"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build geo lookup\n",
    "geo_lookup = CSV.read(\"data/boulder_geo_lookup.csv\")[:,[\"bg\", \"trt\"]];\n",
    "geo_lookup.bg = string.(geo_lookup.bg)\n",
    "geo_lookup.trt = string.(geo_lookup.trt)\n",
    "first(geo_lookup, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure tract IDs between `constraints_trt` and `geo_lookup` are consistent\n",
    "tix = indexin(unique(geo_lookup.trt), string.(constraints_trt.GEOID));\n",
    "constraints_trt = constraints_trt[tix,:];\n",
    "println(sum(unique(geo_lookup.bg) .== string.(constraints_bg.GEOID)) == nrow(constraints_bg))\n",
    "sum(unique(geo_lookup.trt) .== string.(constraints_trt.GEOID)) == nrow(constraints_trt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PUMS response ids\n",
    "serial = collect(constraints_ind.pid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PUMS sample weights\n",
    "wt = collect(constraints_ind.wt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## population and sample size\n",
    "N = sum(constraints_bg.POP);\n",
    "n = nrow(constraints_ind);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual (PUMS) Constraints\n",
    "\n",
    "Isolate the individual constraints and store them in a matrix `pX`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Individual-level (PUMS) constraints\n",
    "excl = [\"pid\", \"wt\"]\n",
    "constraint_cols = [i ∉ excl for i in names(constraints_ind)];\n",
    "pX = constraints_ind[!,constraint_cols];\n",
    "pX = convert(Matrix, pX);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Constraints\n",
    "\n",
    "First, ID the constraint columns representing estimates and the constraint columns representing standard errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_cols_bg = [!endswith(i, 's') && i != \"GEOID\" for i in names(constraints_bg)]\n",
    "est_cols_trt = [!endswith(i, 's') && i != \"GEOID\" for i in names(constraints_trt)]\n",
    "se_cols = [endswith(i, 's') for i in names(constraints_bg)]\n",
    "se_cols = names(constraints_bg)[se_cols];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, isolate the geographic constraint estimates `Y`, first at the upper level, then at the target level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = convert(Matrix, constraints_trt[!,est_cols_trt])\n",
    "Y2 = convert(Matrix, constraints_bg[!,est_cols_bg]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then another set of matrices to store the error variances (`V`) based on `se_cols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## error variances\n",
    "V1 = map(x -> x^2, convert(Matrix, constraints_trt[!,se_cols]))\n",
    "V2 = map(x -> x^2, convert(Matrix, constraints_bg[!,se_cols]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking Geographies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to symbolically link the upper and target geographic constraint levels. This is done based on `geo_lookup`.\n",
    "\n",
    "First, generate a **crosswalk** between the upper and target levels (a matrix of $n$ upper units by $m$ target units):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Geographic crosswalk\n",
    "A1 = Int64[]\n",
    "\n",
    "for G in unique(geo_lookup.trt)\n",
    "\n",
    "    blah = zeros(Int64, 1, nrow(constraints_bg))\n",
    "\n",
    "    isG = [occursin(G, g) for g in collect(geo_lookup.bg)]\n",
    "    for i in findall(isG)\n",
    "        blah[i] = 1\n",
    "    end\n",
    "    append!(A1, blah)\n",
    "\n",
    "end\n",
    "\n",
    "A1 = reshape(A1, nrow(constraints_bg), nrow(constraints_trt))\n",
    "A1 = transpose(A1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate an **identity matrix** for the target units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target unit identity matrix\n",
    "A2 = Matrix(I, nrow(constraints_bg), nrow(constraints_bg));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking the PUMS constraints to Geographies\n",
    "\n",
    "#### Model Matrix\n",
    "\n",
    "Next, create the model matrix `X` for the P-MEDM problem. It consists of:\n",
    "\n",
    "1. The Kronecker product of the PUMS constraints `pX` and the geographic crosswalk `A1`.\n",
    "2. The Kronecker product of the PUMS constraints `pX` and the target level identity matrix `A2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit slow\n",
    "X1 = (sparse(pX') ⊗ A1)'\n",
    "X2 = (sparse(pX') ⊗ A2)'\n",
    "X = hcat(sparse(X1), sparse(X2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design Weights\n",
    "\n",
    "The design weights `q` are the prior estimate of the P-MEDM allocation matrix. The assumption is that each PUMS record has an equal probability of being found in each target unit, relative to the sample weight's share of the total population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design weights\n",
    "q = repeat(wt, size(A1)[2]);\n",
    "q = reshape(q, n, size(A1)[2]);\n",
    "q = q / sum(q);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since P-MEDM is a linear problem, we need to vectorize the constraints (`Y`), error variances (`V`), and design weights (`q`) before running it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize geo. constraints (Y) and normalize\n",
    "Y_vec = vcat(vec(Y1), vec(Y2)) / N; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize variances and normalize\n",
    "V_vec = vcat(vec(V1), vec(V2)) * (n / N^2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the design weights\n",
    "q = vec(q');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diagonal matrix of the error variances (`sV`) is also generated to compute the Hessian for the solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diagonal matrix of variances\n",
    "sV = Diagonal(V_vec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convenience function to compute the P_MEDM probabilities from q, X, λ\n",
    "compute_allocation = function(q, X, λ)\n",
    "\n",
    "    qXl = q .* exp.(-X * λ)\n",
    "    p = qXl / sum(qXl)\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective Function\n",
    "f = function(λ)\n",
    "\n",
    "    qXl = exp.(-X * λ) .* q\n",
    "    p = qXl / sum(qXl)\n",
    "\n",
    "    Xp = X' * p\n",
    "    lvl = λ' * (sV * λ);\n",
    "\n",
    "    return (Y_vec' * λ) + log(sum(qXl)) + (0.5 * lvl)\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient function\n",
    "g! = function(G, λ)\n",
    "    \n",
    "    qXl = q .* exp.(-X * λ)\n",
    "    p = qXl / sum(qXl)\n",
    "    Xp = X'p\n",
    "    G[:] = Y_vec + (sV * λ) - Xp\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hessian function\n",
    "h! = function(H, λ)\n",
    "    \n",
    "    qXl = q .* exp.(-X * λ)\n",
    "    p = qXl / sum(qXl)\n",
    "    dp = spdiagm(0 => p)\n",
    "    H[:] = -((X'p) * (p'X)) + (X' * dp * X) + sV\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Solving the P-MEDM Problem\n",
    "\n",
    "We initialize the coefficient estimates $\\lambda$ at 0. When initializing $\\lambda$ at 0, our prior allocation probabilities are equal to `q`. Each successive step in the P-MEDM optimization updates $\\lambda$ and refines the prior allocation probabilities given by `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      "     0     0.000000e+00     3.355729e-02\n",
      " * time: 0.0736839771270752\n",
      "     1    -1.055967e-01     1.761934e-02\n",
      " * time: 3.564579963684082\n",
      "     2    -2.184494e-01     1.313110e-02\n",
      " * time: 6.883098840713501\n",
      "     3    -3.018777e-01     8.503686e-03\n",
      " * time: 11.576177835464478\n",
      "     4    -3.448676e-01     2.491152e-03\n",
      " * time: 14.889793872833252\n",
      "     5    -3.565122e-01     8.592010e-04\n",
      " * time: 18.04591393470764\n",
      "     6    -3.580423e-01     2.600936e-04\n",
      " * time: 22.101778984069824\n",
      "     7    -3.581893e-01     6.033541e-05\n",
      " * time: 25.195271968841553\n",
      "     8    -3.582005e-01     7.145121e-06\n",
      " * time: 28.4721999168396\n",
      "     9    -3.582008e-01     1.856517e-07\n",
      " * time: 31.253708839416504\n",
      "    10    -3.582008e-01     1.785954e-10\n",
      " * time: 32.71721386909485\n"
     ]
    }
   ],
   "source": [
    "using Optim\n",
    "\n",
    "init_λ = zeros(length(Y_vec))\n",
    "\n",
    "opt = optimize(f, g!, h!, init_λ, NewtonTrustRegion(),\n",
    "               Optim.Options(show_trace=true, iterations = 200));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## final coefficients (lambdas/λ)\n",
    "λ = Optim.minimizer(opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Y</th><th>Yhat</th><th>MOE_lower</th><th>MOE_upper</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>10 rows × 4 columns</p><tr><th>1</th><td>4645.0</td><td>4947.0</td><td>4303.0</td><td>4987.0</td></tr><tr><th>2</th><td>7458.0</td><td>7610.55</td><td>6942.0</td><td>7974.0</td></tr><tr><th>3</th><td>3953.0</td><td>4082.61</td><td>3681.0</td><td>4225.0</td></tr><tr><th>4</th><td>2424.0</td><td>2560.43</td><td>2221.0</td><td>2627.0</td></tr><tr><th>5</th><td>5604.0</td><td>5729.38</td><td>5111.0</td><td>6097.0</td></tr><tr><th>6</th><td>3760.0</td><td>4025.78</td><td>3376.0</td><td>4144.0</td></tr><tr><th>7</th><td>6189.0</td><td>6356.08</td><td>5531.0</td><td>6847.0</td></tr><tr><th>8</th><td>7131.0</td><td>7113.02</td><td>6597.0</td><td>7665.0</td></tr><tr><th>9</th><td>5755.0</td><td>5865.87</td><td>5408.0</td><td>6102.0</td></tr><tr><th>10</th><td>3949.0</td><td>4102.31</td><td>3673.0</td><td>4225.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& Y & Yhat & MOE\\_lower & MOE\\_upper\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 4645.0 & 4947.0 & 4303.0 & 4987.0 \\\\\n",
       "\t2 & 7458.0 & 7610.55 & 6942.0 & 7974.0 \\\\\n",
       "\t3 & 3953.0 & 4082.61 & 3681.0 & 4225.0 \\\\\n",
       "\t4 & 2424.0 & 2560.43 & 2221.0 & 2627.0 \\\\\n",
       "\t5 & 5604.0 & 5729.38 & 5111.0 & 6097.0 \\\\\n",
       "\t6 & 3760.0 & 4025.78 & 3376.0 & 4144.0 \\\\\n",
       "\t7 & 6189.0 & 6356.08 & 5531.0 & 6847.0 \\\\\n",
       "\t8 & 7131.0 & 7113.02 & 6597.0 & 7665.0 \\\\\n",
       "\t9 & 5755.0 & 5865.87 & 5408.0 & 6102.0 \\\\\n",
       "\t10 & 3949.0 & 4102.31 & 3673.0 & 4225.0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×4 DataFrame\n",
       "│ Row │ Y       │ Yhat    │ MOE_lower │ MOE_upper │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────┼─────────┼─────────┼───────────┼───────────┤\n",
       "│ 1   │ 4645.0  │ 4947.0  │ 4303.0    │ 4987.0    │\n",
       "│ 2   │ 7458.0  │ 7610.55 │ 6942.0    │ 7974.0    │\n",
       "│ 3   │ 3953.0  │ 4082.61 │ 3681.0    │ 4225.0    │\n",
       "│ 4   │ 2424.0  │ 2560.43 │ 2221.0    │ 2627.0    │\n",
       "│ 5   │ 5604.0  │ 5729.38 │ 5111.0    │ 6097.0    │\n",
       "│ 6   │ 3760.0  │ 4025.78 │ 3376.0    │ 4144.0    │\n",
       "│ 7   │ 6189.0  │ 6356.08 │ 5531.0    │ 6847.0    │\n",
       "│ 8   │ 7131.0  │ 7113.02 │ 6597.0    │ 7665.0    │\n",
       "│ 9   │ 5755.0  │ 5865.87 │ 5408.0    │ 6102.0    │\n",
       "│ 10  │ 3949.0  │ 4102.31 │ 3673.0    │ 4225.0    │"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## inspect results\n",
    "phat = compute_allocation(q, X, λ);\n",
    "phat = reshape(phat, size(A2)[1], size(pX)[1])';\n",
    "\n",
    "Yhat2 = (N * phat)' * pX;\n",
    "\n",
    "phat_trt = (phat * N) * A1';\n",
    "Yhat1 = phat_trt' * pX;\n",
    "\n",
    "Yhat = vcat(vec(Yhat1), vec(Yhat2));\n",
    "\n",
    "Ype = DataFrame(Y = Y_vec * N, Yhat = Yhat, V = V_vec * (N^2/n));\n",
    "\n",
    "#90% MOEs\n",
    "Ype.MOE_lower = Ype.Y - (sqrt.(Ype.V) * 1.645);\n",
    "Ype.MOE_upper = Ype.Y + (sqrt.(Ype.V) * 1.645);\n",
    "\n",
    "first(Ype[:,[\"Y\", \"Yhat\", \"MOE_lower\", \"MOE_upper\"]], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly 4.6% of the 1665 constraints from the P-MEDM results do not fall within the ACS 90% MOEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04624624624624624"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of constraints falling outside 90% MOE\n",
    "sum((Ype.Yhat .< Ype.MOE_lower) + (Ype.Yhat .> Ype.MOE_upper) .>= 1) / nrow(Ype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Population Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target-level (block group) estimates of each group are simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "syp = DataFrame(hcat(serial, (phat * N)))\n",
    "rn = collect(geo_lookup.bg)\n",
    "rn = append!([\"pid\"], rn)\n",
    "rename!(syp, rn);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the 95% Confidence Intervals on the Synthetic Population Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute the P-MEDM allocation as a vector\n",
    "p = compute_allocation(q, X, λ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inverse Hessian of the solution is the variance-covariance matrix of the coefficients $\\lambda$, see https://www.rpubs.com/nnnagle/PMEDM_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute Inverse Hessian\n",
    "H = h!(Array{Float64}(undef, length(λ), length(λ)), λ);\n",
    "covλ = inv(H);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simulate λ\n",
    "using Distributions\n",
    "using Random\n",
    "Random.seed!(808)\n",
    "nsim = 100\n",
    "simλ = []\n",
    "\n",
    "mvn = MvNormal(λ, Matrix(Hermitian(covλ/N)))\n",
    "\n",
    "simλ = rand(mvn, nsim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simulate p\n",
    "psim = []\n",
    "\n",
    "for s in 1:nsim\n",
    "    ps = compute_allocation(q, X, simλ[:,s])\n",
    "    ps = ps * N\n",
    "    append!(psim, ps)\n",
    "end\n",
    "\n",
    "psim = reshape(psim, :, nsim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% confidence interval\n",
    "ci = [quantile(psim[i,:], (0.025, 0.975)) for i in 1:size(psim)[1]]\n",
    "ci = DataFrame(ci)\n",
    "rename!(ci, [\"lower\", \"upper\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the synthetic pop ests (wide to long format)\n",
    "# 2nd arg = melt columns, 3rd arg = id columns\n",
    "sypm = stack(syp, names(syp)[2:size(syp)[2]], :pid);\n",
    "rename!(sypm, [\"pid\", \"geoid\", \"est\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure order matches conf ints\n",
    "sort!(sypm, [:pid]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## append the conf ints\n",
    "res_ci = hcat(sypm, ci);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>pid</th><th>geoid</th><th>est</th><th>lower</th><th>upper</th></tr><tr><th></th><th>Any</th><th>Cat…</th><th>Any</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 5 columns</p><tr><th>1</th><td>1000306a</td><td>80130121011</td><td>0.121192</td><td>0.131494</td><td>0.151479</td></tr><tr><th>2</th><td>1000306a</td><td>80130121012</td><td>0.183809</td><td>0.124105</td><td>0.148392</td></tr><tr><th>3</th><td>1000306a</td><td>80130121013</td><td>0.118596</td><td>0.120327</td><td>0.146623</td></tr><tr><th>4</th><td>1000306a</td><td>80130121014</td><td>0.141434</td><td>0.0936037</td><td>0.115228</td></tr><tr><th>5</th><td>1000306a</td><td>80130121021</td><td>0.153009</td><td>0.176153</td><td>0.202539</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& pid & geoid & est & lower & upper\\\\\n",
       "\t\\hline\n",
       "\t& Any & Cat… & Any & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1000306a & 80130121011 & 0.121192 & 0.131494 & 0.151479 \\\\\n",
       "\t2 & 1000306a & 80130121012 & 0.183809 & 0.124105 & 0.148392 \\\\\n",
       "\t3 & 1000306a & 80130121013 & 0.118596 & 0.120327 & 0.146623 \\\\\n",
       "\t4 & 1000306a & 80130121014 & 0.141434 & 0.0936037 & 0.115228 \\\\\n",
       "\t5 & 1000306a & 80130121021 & 0.153009 & 0.176153 & 0.202539 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×5 DataFrame\n",
       "│ Row │ pid      │ geoid       │ est      │ lower     │ upper    │\n",
       "│     │ \u001b[90mAny\u001b[39m      │ \u001b[90mCat…\u001b[39m        │ \u001b[90mAny\u001b[39m      │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m  │\n",
       "├─────┼──────────┼─────────────┼──────────┼───────────┼──────────┤\n",
       "│ 1   │ 1000306a │ 80130121011 │ 0.121192 │ 0.131494  │ 0.151479 │\n",
       "│ 2   │ 1000306a │ 80130121012 │ 0.183809 │ 0.124105  │ 0.148392 │\n",
       "│ 3   │ 1000306a │ 80130121013 │ 0.118596 │ 0.120327  │ 0.146623 │\n",
       "│ 4   │ 1000306a │ 80130121014 │ 0.141434 │ 0.0936037 │ 0.115228 │\n",
       "│ 5   │ 1000306a │ 80130121021 │ 0.153009 │ 0.176153  │ 0.202539 │"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(res_ci, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we compute the Monte Carlo Coefficient of Variation for all PUMS responses by block group. (_Need to check these results..._)\n",
    "\n",
    "- [Ref1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3337209/)\n",
    "- [Ref2](https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118849972.app2#:~:text=The%20coefficient%20of%20variance%20is,criterion%20in%20Monte%20Carlo%20simulation.&text=Generating%20a%20random%20number%20is%20a%20key%20step%20in%20Monte%20Carlo%20simulation.&text=In%20principle%2C%20a%20pseudo%2Drandom,tested%20to%20assure%20its%20randomness.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo error\n",
    "mce = [std(psim[i,:]) for i in 1:size(psim)[1]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Coefficient of Variation\n",
    "mcv = p ./ mce;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "minimum(mcv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "maximum(mcv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mean(mcv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "median(mcv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quantile(mcv, 0.25)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quantile(mcv, 0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
