{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-MEDM Problem and Data\n",
    "\n",
    "- Census microdata (PUMS) with $n$ = 15 respondents, $N$ = 1286 people.\n",
    "- Public-Use Microdata Area (PUMA) consisting of 10 block groups.\n",
    "- **PROBLEM**: PUMS provides no information on the block groups in which each \"person\" (sample weight unit) in the PUMS resides. \n",
    "\n",
    "**Solution: Iterative Proportional Fitting (IPF).** Use census data published for two spatial scales, the target units (block groups) and an upper level (tracts), known as _constraints_ to make a \"guess\" at where people described by each response type belong. This is done by comparing the published constraints to synthetic constraints reconstructed from each \"guess\" at the probabilities of people's locations given by P-MEDM. The synthetic constraints are updated iteratively until a \"best guess\" is reached. \n",
    "\n",
    "But...**another problem!** Our census data comes from the American Community Survey (ACS). ACS data is updated annually but is inherently uncertain because it is based on a sample vs. a complete count of the population (i.e., the Decennial Census). \n",
    "\n",
    "**Penalized Maximum-Entropy Dasymetric Modeling (P-MEDM)** is an IPF technique specialized to deal with uncertain census data like the ACS (Leyk et al xx; Nagle et al 2014). Besides taking errors between the published and synthetic constraints into account, P-MEDM also accounts for _error variances_. The more \"in the ballpark\" of the published constraints and their variances the synthetic constraints are, the better the solution. Conversely, if a synthetic constraint is out of the range of probable values (i.e., 90% margin of error) of a published constraint, the more _penalized_ the solution will be.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>GEOID</th><th>POP</th><th>CONST1</th><th>CONST2</th><th>CONST3</th><th>POPs</th><th>CONST1s</th><th>CONST2s</th><th>CONST3s</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3 rows × 9 columns</p><tr><th>1</th><td>10</td><td>344</td><td>152</td><td>101</td><td>175</td><td>9.43398</td><td>7.81025</td><td>12.3288</td><td>11.1803</td></tr><tr><th>2</th><td>20</td><td>260</td><td>84</td><td>106</td><td>107</td><td>4.24264</td><td>7.61577</td><td>8.24621</td><td>7.81025</td></tr><tr><th>3</th><td>30</td><td>682</td><td>200</td><td>242</td><td>293</td><td>8.3666</td><td>12.49</td><td>14.3527</td><td>10.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& GEOID & POP & CONST1 & CONST2 & CONST3 & POPs & CONST1s & CONST2s & CONST3s\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 10 & 344 & 152 & 101 & 175 & 9.43398 & 7.81025 & 12.3288 & 11.1803 \\\\\n",
       "\t2 & 20 & 260 & 84 & 106 & 107 & 4.24264 & 7.61577 & 8.24621 & 7.81025 \\\\\n",
       "\t3 & 30 & 682 & 200 & 242 & 293 & 8.3666 & 12.49 & 14.3527 & 10.0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3×9 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ GEOID │ POP   │ CONST1 │ CONST2 │ CONST3 │ POPs    │ CONST1s │ CONST2s │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m  │ \u001b[90mInt64\u001b[39m  │ \u001b[90mInt64\u001b[39m  │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼───────┼───────┼────────┼────────┼────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 10    │ 344   │ 152    │ 101    │ 175    │ 9.43398 │ 7.81025 │ 12.3288 │\n",
       "│ 2   │ 20    │ 260   │ 84     │ 106    │ 107    │ 4.24264 │ 7.61577 │ 8.24621 │\n",
       "│ 3   │ 30    │ 682   │ 200    │ 242    │ 293    │ 8.3666  │ 12.49   │ 14.3527 │"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read in data\n",
    "constraints_ind = CSV.read(\"data/toy_constraints_ind.csv\");\n",
    "constraints_bg = CSV.read(\"data/toy_constraints_bg.csv\");\n",
    "constraints_trt = CSV.read(\"data/toy_constraints_trt.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>bg</th><th>trt</th></tr><tr><th></th><th>String</th><th>String</th></tr></thead><tbody><p>10 rows × 2 columns</p><tr><th>1</th><td>101</td><td>10</td></tr><tr><th>2</th><td>102</td><td>10</td></tr><tr><th>3</th><td>103</td><td>10</td></tr><tr><th>4</th><td>201</td><td>20</td></tr><tr><th>5</th><td>202</td><td>20</td></tr><tr><th>6</th><td>301</td><td>30</td></tr><tr><th>7</th><td>302</td><td>30</td></tr><tr><th>8</th><td>303</td><td>30</td></tr><tr><th>9</th><td>304</td><td>30</td></tr><tr><th>10</th><td>305</td><td>30</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& bg & trt\\\\\n",
       "\t\\hline\n",
       "\t& String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & 101 & 10 \\\\\n",
       "\t2 & 102 & 10 \\\\\n",
       "\t3 & 103 & 10 \\\\\n",
       "\t4 & 201 & 20 \\\\\n",
       "\t5 & 202 & 20 \\\\\n",
       "\t6 & 301 & 30 \\\\\n",
       "\t7 & 302 & 30 \\\\\n",
       "\t8 & 303 & 30 \\\\\n",
       "\t9 & 304 & 30 \\\\\n",
       "\t10 & 305 & 30 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×2 DataFrame\n",
       "│ Row │ bg     │ trt    │\n",
       "│     │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m │\n",
       "├─────┼────────┼────────┤\n",
       "│ 1   │ 101    │ 10     │\n",
       "│ 2   │ 102    │ 10     │\n",
       "│ 3   │ 103    │ 10     │\n",
       "│ 4   │ 201    │ 20     │\n",
       "│ 5   │ 202    │ 20     │\n",
       "│ 6   │ 301    │ 30     │\n",
       "│ 7   │ 302    │ 30     │\n",
       "│ 8   │ 303    │ 30     │\n",
       "│ 9   │ 304    │ 30     │\n",
       "│ 10  │ 305    │ 30     │"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build geo lookup\n",
    "bg_id = string.(collect(constraints_bg[!,1]));\n",
    "trt_id = [s[1:2] for s in bg_id];\n",
    "geo_lookup = DataFrame(bg = bg_id, trt = trt_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PUMS response ids\n",
    "serial = collect(constraints_ind.SERIAL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PUMS sample weights\n",
    "wt = collect(constraints_ind.PERWT);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## population and sample size\n",
    "N = sum(constraints_bg.POP);\n",
    "n = nrow(constraints_ind);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Individual-level (PUMS) constraints\n",
    "excl = [\"SERIAL\", \"PERWT\"];\n",
    "constraint_cols = [i ∉ excl for i in names(constraints_ind)];\n",
    "pX = constraints_ind[!,constraint_cols];\n",
    "pX = convert(Matrix, pX);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## geographic constraints\n",
    "est_cols_bg = [!endswith(i, 's') && i != \"GEOID\" for i in names(constraints_bg)]\n",
    "est_cols_trt = [!endswith(i, 's') && i != \"GEOID\" for i in names(constraints_trt)]\n",
    "Y1 = convert(Matrix, constraints_trt[!,est_cols_trt])\n",
    "Y2 = convert(Matrix, constraints_bg[!,est_cols_bg]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## error variances\n",
    "se_cols = [endswith(i, 's') for i in names(constraints_bg)];\n",
    "se_cols = names(constraints_bg)[se_cols];\n",
    "V1 = map(x -> x^2, convert(Matrix, constraints_trt[!,se_cols]));\n",
    "V2 = map(x -> x^2, convert(Matrix, constraints_bg[!,se_cols]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×10 LinearAlgebra.Transpose{Any,Array{Any,2}}:\n",
       " 1  1  1  0  0  0  0  0  0  0\n",
       " 0  0  0  1  1  0  0  0  0  0\n",
       " 0  0  0  0  0  1  1  1  1  1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Geographic crosswalk\n",
    "A1 = [];\n",
    "\n",
    "for G in unique(geo_lookup.trt)\n",
    "\n",
    "    blah = zeros(Int8, 1, nrow(constraints_bg))\n",
    "\n",
    "    isG = [occursin(G, g) for g in collect(geo_lookup.bg)]\n",
    "    for i in findall(isG)\n",
    "        blah[i] = 1\n",
    "    end\n",
    "    append!(A1, blah)\n",
    "\n",
    "end\n",
    "\n",
    "A1 = reshape(A1, nrow(constraints_bg), nrow(constraints_trt));\n",
    "A1 = transpose(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Bool,2}:\n",
       " 1  0  0  0  0  0  0  0  0  0\n",
       " 0  1  0  0  0  0  0  0  0  0\n",
       " 0  0  1  0  0  0  0  0  0  0\n",
       " 0  0  0  1  0  0  0  0  0  0\n",
       " 0  0  0  0  1  0  0  0  0  0\n",
       " 0  0  0  0  0  1  0  0  0  0\n",
       " 0  0  0  0  0  0  1  0  0  0\n",
       " 0  0  0  0  0  0  0  1  0  0\n",
       " 0  0  0  0  0  0  0  0  1  0\n",
       " 0  0  0  0  0  0  0  0  0  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Target unit identity matrix\n",
    "A2 = Matrix(I, nrow(constraints_bg), nrow(constraints_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution space (X-matrix)\n",
    "X1 = kron(transpose(pX), A1);\n",
    "X2 = kron(transpose(pX), A2);\n",
    "X = transpose(vcat(X1, X2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design weights\n",
    "q = repeat(wt, size(A1)[2]);\n",
    "q = reshape(q, n, size(A1)[2]);\n",
    "q = q / sum(q);\n",
    "q = vec(q');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize geo. constraints (Y) and normalize\n",
    "Y_vec = vcat(vec(Y1), vec(Y2)) / N; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize variances and normalize\n",
    "V_vec = vcat(vec(V1), vec(V2)) * (n / N^2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diagonal matrix of variances\n",
    "sV = Diagonal(V_vec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the P_MEDM probabilities from q, X, λ\n",
    "compute_allocation = function(q, X, λ)\n",
    "\n",
    "    a0 = exp.(-X * λ)\n",
    "\n",
    "    a = a0 .* q;\n",
    "\n",
    "    b = q' * a0\n",
    "\n",
    "    a/b\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primal Function\n",
    "penalized_entropy = function(w, d, n, N, v)\n",
    "\n",
    "    e = d - w\n",
    "\n",
    "    penalty = (e^2 / (2. * v))\n",
    "\n",
    "    ent = ((n / N) * (w / d) * log((w/d)))\n",
    "\n",
    "    pe = (-1. * ent) - penalty\n",
    "\n",
    "    return pe\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective Function\n",
    "neg_pe = function(λ)\n",
    "\n",
    "    phat = compute_allocation(q, X, λ)\n",
    "    phat = reshape(phat, size(A2)[1], size(pX)[1])'\n",
    "\n",
    "    Yhat2 = (N * phat)' * pX\n",
    "\n",
    "    phat_trt = (phat * N) * A1'\n",
    "    Yhat1 = phat_trt' * pX\n",
    "\n",
    "    Yhat = vcat(vec(Yhat1), vec(Yhat2))\n",
    "\n",
    "    Ype = DataFrame(Y = Y_vec * N, Yhat = Yhat, V = V_vec * (N^2/n))\n",
    "\n",
    "    pe = penalized_entropy.(Ype.Y, Ype.Yhat, n, N, Ype.V)\n",
    "\n",
    "    -1. * mean(pe)\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer: [5.36e-01, -3.61e-02, -5.00e-01,  ...]\n",
       "    Minimum:   -1.553683e-06\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "    Initial Point: [0.00e+00, 0.00e+00, 0.00e+00,  ...]\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 1.48e-08 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 5.33e-09 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 1.83e-16 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 1.18e-10 ≰ 0.0e+00\n",
       "    |g(x)|                 = 5.04e-09 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   1  (vs limit Inf)\n",
       "    Iterations:    99\n",
       "    f(x) calls:    281\n",
       "    ∇f(x) calls:   281\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optimize(neg_pe, zeros(length(Y_vec)), BFGS(), autodiff = :forward,\n",
    "            Optim.Options(iterations = 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## final coefficients (lambdas/λ)\n",
    "λ = Optim.minimizer(opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## inspect results\n",
    "phat = compute_allocation(q, X, λ);\n",
    "phat = reshape(phat, size(A2)[1], size(pX)[1])';\n",
    "\n",
    "Yhat2 = (N * phat)' * pX;\n",
    "\n",
    "phat_trt = (phat * N) * A1';\n",
    "Yhat1 = phat_trt' * pX;\n",
    "\n",
    "Yhat = vcat(vec(Yhat1), vec(Yhat2));\n",
    "\n",
    "Ype = DataFrame(Y = Y_vec * N, Yhat = Yhat, V = V_vec * (N^2/n));\n",
    "\n",
    "#90% MOEs\n",
    "Ype.MOE_lower = Ype.Y - (sqrt.(Ype.V) * 1.645);\n",
    "Ype.MOE_upper = Ype.Y + (sqrt.(Ype.V) * 1.645);\n",
    "\n",
    "# Proportion of contstraints falling outside 90% MOE\n",
    "sum((Ype.Yhat .< Ype.MOE_lower) + (Ype.Yhat .> Ype.MOE_upper) .>= 1) / nrow(Ype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
